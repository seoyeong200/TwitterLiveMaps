{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 스트리밍 실습 : 카프카 스트리밍 애플리케이션\n",
    "\n",
    "> 플루언트디를 통해 생성된 `movies` 토픽의 메시지를 스파크 스트리밍 애플리케이션을 통해 카프카 토픽을 처리합니다\n",
    "\n",
    "## 학습 목표\n",
    "* `Kafka`에 저장된 스트리밍 데이터를 처리합니다\n",
    "  - 카프카 메시지 프로듀서를 통해서 카프카에 스트림 데이터를 생성합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5bfdae64f6df:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5948a7b460>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "# 현재 기동된 스파크 애플리케이션의 포트를 확인하기 위해 스파크 정보를 출력합니다\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트림 테이블을 주기적으로 조회하는 함수 (name: 이름, sql: Spark SQL, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStream(name, sql, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)              # 출력 Cell 을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Query: '+sql)\n",
    "        display(spark.sql(sql))              # Spark SQL 을 수행합니다\n",
    "        sleep(sleep_secs)                    # sleep_secs 초 만큼 대기합니다\n",
    "        i += 1\n",
    "\n",
    "# 스트림 쿼리의 상태를 주기적으로 조회하는 함수 (name: 이름, query: Streaming Query, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)      # Output Cell 의 내용을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)  # 마지막 수행된 쿼리의 상태를 출력합니다\n",
    "        sleep(sleep_secs)            # 지정된 시간(초)을 대기합니다\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td></tr>\n",
       "<tr><td>3</td><td>엘지에너지솔루션</td></tr>\n",
       "<tr><td>100</td><td>엘지</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+----------------+\n",
       "|emp_id|        emp_name|\n",
       "+------+----------------+\n",
       "|     1|        엘지전자|\n",
       "|     2|        엘지화학|\n",
       "|     3|엘지에너지솔루션|\n",
       "|   100|            엘지|\n",
       "+------+----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filePath = f\"{work_data}/json\"\n",
    "fileJson = spark.read.json(filePath)\n",
    "fileJson.printSchema() # 기본적으로 스키마를 추론하는 경우 integer 가 아니라 long 으로 잡는다\n",
    "display(fileJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[fileSource] Iteration: 3, Query: select * from fileSource'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td></tr>\n",
       "<tr><td>3</td><td>엘지에너지솔루션</td></tr>\n",
       "<tr><td>100</td><td>엘지</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+----------------+\n",
       "|emp_id|        emp_name|\n",
       "+------+----------------+\n",
       "|     1|        엘지전자|\n",
       "|     2|        엘지화학|\n",
       "|     3|엘지에너지솔루션|\n",
       "|   100|            엘지|\n",
       "+------+----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filePath = f\"{work_data}/json\"\n",
    "queryName = \"fileSource\"\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "fileSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"emp_id\", LongType()))\n",
    "    .add(StructField(\"emp_name\", StringType()))\n",
    ")\n",
    "fileReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"json\")\n",
    "    .schema(fileSchema)\n",
    "    .load(filePath)\n",
    ")\n",
    "fileSelector = fileReader.select(\"emp_id\", \"emp_name\")\n",
    "fileWriter = (\n",
    "    fileSelector\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"update\")\n",
    ")\n",
    "fileTrigger = (\n",
    "    fileWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "fileQuery = fileTrigger.start()\n",
    "displayStream(\"fileSource\", f\"select * from {queryName}\", 3, 3)\n",
    "fileQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka 소스 스트리밍 실습\n",
    "\n",
    "#### 1. 카프카에 데이터 쓰기 - producer.py\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import names\n",
    "\n",
    "# 카프카에 데이터를 전송하는 코드\n",
    "def produce(port):\n",
    "    try:\n",
    "        hostname=\"kafka:%d\" % port\n",
    "        producer = KafkaProducer(bootstrap_servers=[hostname],\n",
    "            value_serializer=lambda x: dumps(x).encode('utf-8')\n",
    "        )\n",
    "        data = {}\n",
    "        for seq in range(9999):\n",
    "            print(\"Sequence\", seq)\n",
    "            first_name = names.get_first_name()\n",
    "            last_name = names.get_last_name()\n",
    "            data[\"first\"] = first_name\n",
    "            data[\"last\"] = last_name\n",
    "            producer.send('events', value=data)\n",
    "            sleep(0.5)\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc(sys.stdout)\n",
    "\n",
    "# 카프카의 데이터 수신을 위한 내부 포트는 9093\n",
    "if __name__ == \"__main__\":\n",
    "    port = 9093\n",
    "    if len(sys.argv) == 2:\n",
    "        port = int(sys.argv[1])\n",
    "    produce(port)\n",
    "```\n",
    "#### 2. 터미널에 접속하여 카프카 메시지를 생성합니다\"\n",
    "```bash\n",
    "(base) # python producer.py\n",
    "```\n",
    "\n",
    "#### 3. 스파크를 통해 생성된 스트리밍 데이터를 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 컨테이너 내부에서 토픽에 접근하기 위한 포트는 9093\u001b[39;00m\n\u001b[1;32m      2\u001b[0m kafkaReader \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mspark\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mreadStream\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka:9093\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovies\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m kafkaSchema \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     12\u001b[0m     StructType()\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39madd(StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39madd(StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrade\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()))\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m kafkaSelector \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     21\u001b[0m     kafkaReader\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovies.title as title\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovies.year as year\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 컨테이너 내부에서 토픽에 접근하기 위한 포트는 9093\n",
    "kafkaReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "    .option(\"subscribe\", \"movies\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "kafkaSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"movie\", StringType()))\n",
    "    .add(StructField(\"title\", StringType()))\n",
    "    .add(StructField(\"title_eng\", StringType()))\n",
    "    .add(StructField(\"year\", IntegerType()))\n",
    "    .add(StructField(\"grade\", StringType()))\n",
    ")\n",
    "kafkaSelector = (\n",
    "    kafkaReader\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"movies\")\n",
    "    )\n",
    "    .selectExpr(\"movies.title as title\", \"movies.year as year\")\n",
    "    .groupBy(\"year\")\n",
    "    .count().alias(\"count\")\n",
    ")\n",
    "\n",
    "queryName = \"kafkaSource\"\n",
    "kafkaWriter = (\n",
    "    kafkaSelector\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "kafkaTrigger = (\n",
    "    kafkaWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "kafkaQuery = kafkaTrigger.start()\n",
    "displayStream(\"kafkaSource\", f\"select year, count from {queryName} order by count desc limit 5\", 30, 3)\n",
    "kafkaQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaQuery.status\n",
    "kafkaQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컨테이너 내부에서 토픽에 접근하기 위한 포트는 9093\n",
    "kafkaReader = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "    .option(\"subscribe\", \"movies\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "kafkaSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"movie\", StringType()))\n",
    "    .add(StructField(\"title\", StringType()))\n",
    "    .add(StructField(\"title_eng\", StringType()))\n",
    "    .add(StructField(\"year\", IntegerType()))\n",
    "    .add(StructField(\"grade\", StringType()))\n",
    ")\n",
    "\n",
    "kafkaSelector = (\n",
    "    kafkaReader\n",
    "    .select(\n",
    "        from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"movies\")\n",
    "    )\n",
    "    .selectExpr(\"movies.movie as id\", \"movies.title as title\", \"movies.grade as grade\", \"movies.year as year\")\n",
    ")\n",
    "\n",
    "queryName = \"kafkaSink\"\n",
    "kafkaWriter = (\n",
    "    kafkaSelector\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "    .option(\"topic\", \"kor_movies\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "kafkaTrigger = (\n",
    "    kafkaWriter\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "kafkaQuery = kafkaTrigger.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Terminated with exception: org/apache/spark/sql/internal/connector/SupportsStreamingUpdate',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
